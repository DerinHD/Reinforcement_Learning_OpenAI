from abc import ABC, abstractmethod
import os
import pickle
import gymnasium as gym
import numpy

class BaseEnvironment(ABC, gym.Env):
    """
    This abstract class serves as an interface for defining reinforcement learning environments based on the OpenAI gymnasium library. It inherits
    from the ABC class and the Env class from the gymnasium package. Whenever a new environemnt is integrated to the system, it needs to inherit from 
    the base class.

    Attributes:

    current_state (StateType):
        Current state of the environment 

    feature_size (int):
        Size of feature vector phi(s,a)

    theta (numpy.ndarray):
        Weight vector which is used to compute the reward for a (state, action) R(s,a) = theta^T phi(s,a)

    reward_function_type (str):
        Name of the reward function. At the current moment there are two options:
            "Default": Use standard reward function reeceived from the environment
            "Max_Entropy": Use reconstructed reward function generated by the max entropy algorithm
    """

    def __init__(self): 
        """
        Initialize environment

        Args:
        
        reward_function (str):
            Name of the reward function. At the current moment, there are two options: Default or MaxEntropy. 

        action_space (any):
            Action space (Discrete, Box, etc.)

        observation_space (any):
            Observation space (Discrete, Box, etc.)

        parameters (dict):
            parameters of the environment
        """
        self.reward_Function ="Default"
        
        self.render_mode = None
        self.action_space = None
        self.observation_space = None

        self.parameters = None
        self.theta = None
        self.current_state = None
        self.feature_size = None
        
    def set_reward_function(self, reward_Function:str):
        rewardFunctionsPossible = ["Default", "Max_Entropy"]

        if reward_Function not in rewardFunctionsPossible:
            self.reward_Function = "Default"
        else:
            self.reward_Function = reward_Function

    def perform_step(self, action):
        """
        Perform an action in the environment. Additionally, use another reward function.

        Args:
        
        action (ActionType):
            Next action

        Returns:
        
        next_state (StateType):
            Next state received from the environment

        reward (SupportsFloat):
            Reward (Does not need to be the one received by the environment)

        termination (bool):
            Boolean indicating the termination of the episode

        truncation (bool):
            Boolean indicating the truncation of the episode (e.g. maximum number of steps possible)
        
        info (dict[str,Any]): 
            Additional info about taking a step
        """

        next_state, reward , termination, truncation, info = self.step(action) # perform step 

        rewardUsed = reward

        if self.reward_Function == "MaxEntropy":
            feature= self.transform_state_action_to_feature(self.current_state, action)
            reward = feature @ self.theta

        if self.round == 500:
            truncation = True

        self.round += 1
        self.current_state = next_state   

        return next_state, rewardUsed, termination, truncation, info
    
    def transform_state_action_to_feature(self, state, action):
        """
        Transforms a (state, action) pair into feature vector. This function can be used to
        reduce the state complexity of the environment or for inverse Reinforcement learning.

        Args:

        state (StateType):
            State

        action (ActionType):
            Action

        Returns:
        
        feature_vector (numpy.ndarray):
            feature_vector
        """
        raise NotImplementedError
    
    def max_entropy_configure(self, theta: numpy.ndarray):
        """
        configure the weight vector of the max entropy method

        Args:

        theta (numpy.ndarray):
            Weight vector which is used to compute the reward for a (state, action) R(s,a) = theta^T phi(s,a)
        """

        self.theta = theta

    def save_environment(self, file_path: str):
        """
        Saves the parameters of the environment in a file. Note that the whole environment class is not serializable.

        Args:

        file_path:
            Path to file where environment data will be stored
        """
        
        with open(file_path, 'wb') as file:
            pickle.dump(self.parameters, file)

    @classmethod 
    def load_environment(cls, file_path: str, render_mode = "human"): 
        """
        Loads the environment

        Args:

        file_path:
            Path to file where the environment data will be loaded from.

        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"The file {file_path} does not exist.")

        with open(file_path, 'rb') as file:
            parameters = pickle.load(file)

            parameters["render_mode"] = render_mode

            return cls(**parameters)
            
    @abstractmethod
    def create_environment(render_mode = "human"):
        """
        Creates an environment via terminal. 

        Returns:
            environment (BaseEnvironment):
                Environment
        """